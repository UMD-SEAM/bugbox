% -------------------------------------------------------------------------
% This BibTex file was generated by Qiqqa (http://www.qiqqa.com/?ref=EXPBIB)
% Thursday, June 27, 2013 9:36:25 PM
% Version 3
% -------------------------------------------------------------------------

@INPROCEEDINGS{testbedpaper
,	author	= {Stuckman, J. and Purtilo, J.}
,	title	= {A Testbed for the Evaluation of Web Intrusion Prevention Systems}
,	booktitle	= {Proceedings of MetriSec 2011 International Workshop on Security Measurements and Metrics}
,	year	= {2011}
,	pages	= {66 -75}
,	publisher	= {IEEE}
,	month	= {Sept}
,	abstract	= {Web intrusion prevention systems are popular for defending web applications against common attacks, such as SQL injection and cross-site scripting, but a standardized methodology to evaluate and benchmark such systems is not available. We outline several requirements for a testing and evaluation framework for these systems, and we introduce the concept of a benchmarking testbed, which automatically performs the evaluation in a standardized and reproducible way. By allowing benchmarks to draw from a corpus of installable modules which can be based on actual security vulnerabilities, members of the security community can continuously maintain and improve the benchmark, allowing it to be updated as threats and defenses evolve. We developed a prototype of this testbed and determined that the testbed should automate several common web testing tasks on behalf of its modules in order to ease module development. Although our experiences with the prototype suggest that developing such a testbed is viable, we identified several open questions related to benchmark coverage and performance measurement that should be resolved in order for the resulting benchmark to be useful to end users.}
,	keywords	= {SQL injection attack;Web application;Web intrusion prevention system;Web testing task;benchmarking testbed concept;cross-site scripting attck;module development;security vulnerability;Internet;security of data;}
,	doi	= {10.1109/Metrisec.2011.14}
}

@misc{NVD
,	title	= {National Vulnerability Database}
,              howpublished	= {http://nvd.nist.gov/}
, key={NVD}
}

@misc{exploitdb
,	title	= {Exploit DB}
,	howpublished	= {http://www.exploit-db.com/}
}

@INPROCEEDINGS{mappingfaults
,	author	= {Fonseca, J. and Vieira, M.}
,	title	= {Mapping software faults with web security vulnerabilities}
,	booktitle	= {Proceedings of IEEE DSN '08 Conference on Dependable Systems and Networks}
,	year	= {2008}
,	pages	= {257 -266}
,	abstract	= {Web applications are typically developed with hard time constraints and are often deployed with critical software bugs, making them vulnerable to attacks. The classification and knowledge of the typical software bugs that lead to security vulnerabilities is of utmost importance. This paper presents a field study analyzing 655 security patches of six widely used web applications. Results are compared against other field studies on general software faults (i.e., faults not specifically related to security), showing that only a small subset of software fault types is related to security. Furthermore, the detailed analysis of the code of the patches has shown that web application vulnerabilities result from software bugs affecting a restricted collection of statements. A detailed analysis of the conditions/locations where each fault was observed in our field study is presented allowing future definition of realistic fault models that cause security vulnerabilities in web applications, which is the key element to design a realistic attack injector.}
,	keywords	= {Web application;Web security vulnerabilities;fault models;software bugs;software faults mapping;Internet;security of data;software fault tolerance;}
,	doi	= {10.1109/DSN.2008.4630094}
}

@INPROCEEDINGS{predictingdefectseclipse
,	author	= {Zimmermann, T. and Premraj, R. and Zeller, A.}
,	title	= {Predicting Defects for Eclipse}
,	booktitle	= {Predictor Models in Software Engineering, 2007. PROMISE'07: ICSE Workshops 2007. International Workshop on}
,	year	= {2007}
,	pages	= {9}
,	month	= {may}
,	abstract	= {We have mapped defects from the bug database of eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.}
,	keywords	= {Eclipse;bug database;common complexity metrics;defect prediction models;open-source projects;source code locations;program debugging;public domain software;}
,	doi	= {10.1109/PROMISE.2007.10}
}

@INPROCEEDINGS{onetechnique
,	author	= {Austin, A. and Williams, L.}
,	title	= {One Technique is Not Enough: A Comparison of Vulnerability Discovery Techniques}
,	booktitle	= {Empirical Software Engineering and Measurement (ESEM), 2011 International Symposium on}
,	year	= {2011}
,	pages	= {97 -106}
,	month	= {sept.}
,	abstract	= {Security vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. To ease this difficulty, many different types of techniques have been devised to aid developers in vulnerability discovery. The goal of this research is to improve vulnerability detection by comparing the effectiveness of vulnerability discovery techniques and to provide specific recommendations to improve vulnerability discovery with these techniques. We conducted a case study on two electronic health record systems to compare four discovery techniques: systematic and exploratory manual penetration testing, static analysis, and automated penetration testing. In our case study, we found empirical evidence that no single technique discovered every type of vulnerability. We discovered almost no individual vulnerabilities with multiple discovery techniques. We also found that systematic manual penetration testing found the most design flaws, while static analysis found the most implementation bugs. Finally, we found the most effective vulnerability discovery technique in terms of vulnerabilities discovered per hour was automated penetration testing. These results suggest that if one has limited time to preform vulnerability discovery one should conduct automated penetration testing to discover implementation bugs and systematic manual penetration testing to discover design flaws.}
,	keywords	= {automated penetration testing;electronic health record systems;exploratory manual penetration;security vulnerabilities;software developers;software vulnerabilities;static analysis;vulnerability discovery technique comparison;security of data;software engineering;}
,	doi	= {10.1109/ESEM.2011.18}
,	issn	= {1938-6451}
}

@article{stuckman2011analyzing,
  title={Analyzing the wikisphere: Methodology and data to support quantitative wiki research},
  author={Stuckman, Jeffrey and Purtilo, James},
  journal={Journal of the American Society for Information Science and Technology},
  volume={62},
  number={8},
  pages={1564--1576},
  year={2011},
  publisher={Wiley Online Library}
}

@article{evalcomplexityvul
,	author	= {Shin, Yonghee and Meneely, Andrew and Williams, Laurie and Osborne, Jason A}
,	title	= {Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities}
,	journal	= {Software Engineering, IEEE Transactions on}
,	year	= {2011}
,	volume	= {37}
,	number	= {6}
,	pages	= {772--787}
,	publisher	= {IEEE}
}

@misc{OSVDB
,	title	= {{OSVDB}: The Open Source Vulnerability Database}
,	howpublished	= {http://www.osvdb.org/}
, key = {OSVDB}

}

@misc{metasploit,
author = {Rapid7},
title = {MetaSploit Framework},
month = Spring,
year = {2013},
howpublished ="\url{http://www.metasploit.com/}"
}

@inproceedings{defectcorpus
,	author	= {D'Ambros, Marco and Lanza, Michele and Robbes, Romain}
,	title	= {An extensive comparison of bug prediction approaches}
,	booktitle	= {Mining Software Repositories (MSR), 2010 7th IEEE Working Conference on}
,	year	= {2010}
,	pages	= {31--41}
,	organization	= {IEEE}
}

@INPROCEEDINGS{metrisec2012surface
,	author	= {Stuckman, J. and Purtilo, J.}
,	title	= {Comparing and Applying Attack Surface Metrics}
,	booktitle	= {Proceedings of MetriSec 2012: International Workshop on Security Measurements and Metrics}
,	year	= {2012}
}

@ARTICLE{miningbugfinding
,	author	= {Williams, C.C. and Hollingsworth, J.K.}
,	title	= {Automatic mining of source code repositories to improve bug finding techniques}
,	journal	= {Software Engineering, IEEE Transactions on}
,	year	= {2005}
,	volume	= {31}
,	number	= {6}
,	pages	= { 466 - 480}
,	month	= {june}
,	abstract	= { We describe a method to use the source code change history of a software project to drive and help to refine the search for bugs. Based on the data retrieved from the source code repository, we implement a static source code checker that searches for a commonly fixed bug and uses information automatically mined from the source code repository to refine its results. By applying our tool, we have identified a total of 178 warnings that are likely bugs in the Apache Web server source code and a total of 546 warnings that are likely bugs in Wine, an open-source implementation of the Windows API. We show that our technique is more effective than the same static analysis that does not use historical data from the source code repository.}
,	keywords	= { Apache Web server; Windows API; automatic mining; bug finding technique; configuration control; data retrieval; debugging aids; historical data; open-source implementation; software project; source code repository; static analysis; static source code checker; testing tools; version control; Internet; application program interfaces; configuration management; data mining; file servers; program debugging; program diagnostics; program testing; public domain software;}
,	doi	= {10.1109/TSE.2005.63}
,	issn	= {0098-5589}
}

@article{empiricalinvestigation
,	author	= {Huynh, T. and Miller, J.}
,	title	= {An empirical investigation into open source web applicationsâ€™ implementation vulnerabilities}
,	journal	= {Empirical Software Engineering}
,	year	= {2010}
,	volume	= {15}
,	number	= {5}
,	pages	= {556--576}
,	publisher	= {Springer}
}

@misc{SAMATE
,	howpublished	= {http://samate.nist.gov/SRD/}
}

@article{livshits2005defining,
  title={Defining a set of common benchmarks for web application security},
  author={Livshits, Benjamin},
  year={2005},
  publisher={Citeseer},
howpublished={doi:10.1.1.59.6723}
}

@inproceedings{antunes2009comparing,
  title={Comparing the effectiveness of penetration testing and static code analysis on the detection of sql injection vulnerabilities in web services},
  author={Antunes, Nuno and Vieira, Marco},
  booktitle={Dependable Computing, 2009. PRDC'09. 15th IEEE Pacific Rim International Symposium on},
  pages={301--306},
  year={2009},
  organization={IEEE}
}

@inproceedings{bufferoverflowbenchmark
,	author	= {Ku, Kelvin and Hart, Thomas E. and Chechik, Marsha and Lie, David}
,	title	= {A buffer overflow benchmark for software model checkers}
,	booktitle	= {Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering}
,	year	= {2007}
,	pages	= {389--392}
,	publisher	= {ACM}
,	address	= {New York, NY, USA}
,	series	= {ASE '07}
,	isbn	= {978-1-59593-882-4}
,	location	= {Atlanta, Georgia, USA}
,	numpages	= {4}
,	url	= {http://doi.acm.org/10.1145/1321631.1321691}
,	doi	= {http://doi.acm.org/10.1145/1321631.1321691}
,	acmid	= {1321691}
,	keywords	= {array bounds checking, benchmark, buffer overflow, model checking}
}

@inproceedings{commandinjection
,	author	= {Su, Zhendong and Wassermann, Gary}
,	title	= {The essence of command injection attacks in web applications}
,	booktitle	= {Conference record of the 33rd ACM SIGPLAN-SIGACT symposium on Principles of programming languages}
,	year	= {2006}
,	pages	= {372--382}
,	publisher	= {ACM}
,	address	= {New York, NY, USA}
,	series	= {POPL '06}
,	isbn	= {1-59593-027-2}
,	location	= {Charleston, South Carolina, USA}
,	numpages	= {11}
,	url	= {http://doi.acm.org/10.1145/1111037.1111070}
,	doi	= {http://doi.acm.org/10.1145/1111037.1111070}
,	acmid	= {1111070}
,	keywords	= {command injection attacks, grammars, parsing, runtime verification, web applications}
}

