% -------------------------------------------------------------------------
% This BibTex file was generated by Qiqqa (http://www.qiqqa.com/?ref=EXPBIB)
% Thursday, June 27, 2013 10:30:21 PM
% Version 3
% -------------------------------------------------------------------------

@misc{exploitdb
,	title	= {Exploit DB}
,	author	= {Offensive Security}
,	howpublished	= {\url{http://www.exploit-db.com/}}
}

@inproceedings{antunes2009comparing
,	author	= {Antunes, Nuno and Vieira, Marco}
,	title	= {Comparing the effectiveness of penetration testing and static code analysis on the detection of sql injection vulnerabilities in web services}
,	booktitle	= {Proceedings of PRDC'09: 15th IEEE Pacific Rim International Symposium on Dependable Computing.}
,	year	= {2009}
,	pages	= {301--306}
,	organization	= {IEEE}
}

@INPROCEEDINGS{onetechnique
,	author	= {Austin, A. and Williams, L.}
,	title	= {One Technique is Not Enough: A Comparison of Vulnerability Discovery Techniques}
,	booktitle	= {Proceedings of Empirical Software Engineering and Measurement (ESEM), International Symposium on}
,	year	= {2011}
,	pages	= {97 -106}
,	month	= {Sept}
,	abstract	= {Security vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. To ease this difficulty, many different types of techniques have been devised to aid developers in vulnerability discovery. The goal of this research is to improve vulnerability detection by comparing the effectiveness of vulnerability discovery techniques and to provide specific recommendations to improve vulnerability discovery with these techniques. We conducted a case study on two electronic health record systems to compare four discovery techniques: systematic and exploratory manual penetration testing, static analysis, and automated penetration testing. In our case study, we found empirical evidence that no single technique discovered every type of vulnerability. We discovered almost no individual vulnerabilities with multiple discovery techniques. We also found that systematic manual penetration testing found the most design flaws, while static analysis found the most implementation bugs. Finally, we found the most effective vulnerability discovery technique in terms of vulnerabilities discovered per hour was automated penetration testing. These results suggest that if one has limited time to preform vulnerability discovery one should conduct automated penetration testing to discover implementation bugs and systematic manual penetration testing to discover design flaws.}
,	keywords	= {automated penetration testing;electronic health record systems;exploratory manual penetration;security vulnerabilities;software developers;software vulnerabilities;static analysis;vulnerability discovery technique comparison;security of data;software engineering;}
,	doi	= {10.1109/ESEM.2011.18}
,	issn	= {1938-6451}
}

@ARTICLE{automaticvulsig
,	author	= {Brumley, D. and Newsome, J. and Song, D. and Hao Wang and Jha, S.}
,	title	= {Theory and Techniques for Automatic Generation of Vulnerability-Based Signatures}
,	journal	= {Dependable and Secure Computing, IEEE Transactions on}
,	year	= {2008}
,	volume	= {5}
,	number	= {4}
,	pages	= {224 -241}
,	month	= {Oct-Dec}
,	abstract	= {In this paper, we explore the problem of creating emphvulnerability signatures. A vulnerability signature is based on a program vulnerability, and is not specific to any particular exploit. The advantage of vulnerability signatures is that their quality can be guaranteed. In particular, we create vulnerability signatures which are guaranteed to have zero false positives. We show how to automate signature creation for any vulnerability that can be detected by a runtime monitor. We provide a formal definition of a vulnerability signature, and investigate the computational complexity of creating and matching vulnerability signatures. We systematically explore the design space of vulnerability signatures. We also provide specific techniques for creating vulnerability signatures in a variety of language classes. In order to demonstrate our techniques, we have built a prototype system. Our experiments show that we can, using a single exploit, automatically generate a vulnerability signature as a regular expression, as a small program, or as a system of constraints. We demonstrate techniques for creating signatures of vulnerabilities which can be exploited via multiple program paths. Our results indicate that our approach is a viable option for signature generation, especially when guarantees are desired.}
,	keywords	= {Turing-complete language;automated generation;multiple vulnerable program paths;multiple-path vulnerability;program vulnerability;vulnerability-based signatures;Turing machines;computational complexity;digital signatures;}
,	doi	= {10.1109/TDSC.2008.55}
,	issn	= {1545-5971}
}

@inproceedings{defectcorpus
,	author	= {D'Ambros, Marco and Lanza, Michele and Robbes, Romain}
,	title	= {An extensive comparison of bug prediction approaches}
,	booktitle	= {Proceedings of Mining Software Repositories (MSR), 7th IEEE Working Conference on}
,	year	= {2010}
,	pages	= {31--41}
,	organization	= {IEEE}
}

@INPROCEEDINGS{mappingfaults
,	author	= {Fonseca, J. and Vieira, M.}
,	title	= {Mapping software faults with web security vulnerabilities}
,	booktitle	= {Proceedings of IEEE DSN '08 Conference on Dependable Systems and Networks}
,	year	= {2008}
,	pages	= {257 -266}
,	abstract	= {Web applications are typically developed with hard time constraints and are often deployed with critical software bugs, making them vulnerable to attacks. The classification and knowledge of the typical software bugs that lead to security vulnerabilities is of utmost importance. This paper presents a field study analyzing 655 security patches of six widely used web applications. Results are compared against other field studies on general software faults (i.e., faults not specifically related to security), showing that only a small subset of software fault types is related to security. Furthermore, the detailed analysis of the code of the patches has shown that web application vulnerabilities result from software bugs affecting a restricted collection of statements. A detailed analysis of the conditions/locations where each fault was observed in our field study is presented allowing future definition of realistic fault models that cause security vulnerabilities in web applications, which is the key element to design a realistic attack injector.}
,	keywords	= {Web application;Web security vulnerabilities;fault models;software bugs;software faults mapping;Internet;security of data;software fault tolerance;}
,	doi	= {10.1109/DSN.2008.4630094}
}

@article{empiricalinvestigation
,	author	= {Huynh, T. and Miller, J.}
,	title	= {An empirical investigation into open source web applicationsâ€™ implementation vulnerabilities}
,	journal	= {Empirical Software Engineering}
,	year	= {2010}
,	volume	= {15}
,	number	= {5}
,	pages	= {556--576}
,	publisher	= {Springer}
}

@inproceedings{bufferoverflowbenchmark
,	author	= {Ku, Kelvin and Hart, Thomas E. and Chechik, Marsha and Lie, David}
,	title	= {A buffer overflow benchmark for software model checkers}
,	booktitle	= {Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering}
,	year	= {2007}
,	pages	= {389--392}
,	publisher	= {ACM}
,	address	= {New York, NY, USA}
,	series	= {ASE '07}
,	isbn	= {978-1-59593-882-4}
,	location	= {Atlanta, Georgia, USA}
,	numpages	= {4}
,	url	= {http://doi.acm.org/10.1145/1321631.1321691}
,	doi	= {http://doi.acm.org/10.1145/1321631.1321691}
,	acmid	= {1321691}
,	keywords	= {array bounds checking, benchmark, buffer overflow, model checking}
}

@inproceedings{livshits2005defining
,	author	= {Livshits, Benjamin}
,	title	= {Defining a set of common benchmarks for web application security}
,	year	= {2005}
,	publisher	= {Citeseer}
,	howpublished	= {doi:10.1.1.59.6723}
,	booktitle	= {Workshop on Defining the State of the Art in Software Security Tools}
,	address	= {Baltimore}
,	month	= {August}
}

@misc{NVD
,	title	= {National Vulnerability Database}
,	year	= {2013}
,	howpublished	= {\url{http://nvd.nist.gov/}}
,	key	= {NVD}
,	author	= {NIST}
}

@misc{SAMATE
,	title	= {SAMATE Reference Dataset}
,	howpublished	= {\url{http://samate.nist.gov/SRD/}}
,	author	= {{NIST}}
}

@misc{OSVDB
,	title	= {OSVDB: The Open Source Vulnerability Database}
,	year	= {2013}
,	howpublished	= {\url{http://www.osvdb.org/}}
,	key	= {OSVDB}
,	author	= {Offensive Security}
}

@misc{webgoat
,	title	= {WebGoat}
,	author	= {OWASP}
,	year	= {2013}
,	howpublished	= {\url{https://code.google.com/p/webgoat/}}
}

@misc{metasploit
,	title	= {MetaSploit Framework}
,	author	= {Rapid7}
,	year	= {2013}
,	howpublished	= {\url{http://www.metasploit.com/}}
}

@article{evalcomplexityvul
,	author	= {Shin, Yonghee and Meneely, Andrew and Williams, Laurie and Osborne, Jason A}
,	title	= {Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities}
,	journal	= {Software Engineering, IEEE Transactions on}
,	year	= {2011}
,	volume	= {37}
,	number	= {6}
,	pages	= {772--787}
,	publisher	= {IEEE}
}

@INPROCEEDINGS{testbedpaper
,	author	= {Stuckman, J. and Purtilo, J.}
,	title	= {A Testbed for the Evaluation of Web Intrusion Prevention Systems}
,	booktitle	= {Proceedings of MetriSec: International Workshop on Security Measurements and Metrics}
,	year	= {2011}
,	pages	= {66 -75}
,	publisher	= {IEEE}
,	month	= {Sept}
,	abstract	= {Web intrusion prevention systems are popular for defending web applications against common attacks, such as SQL injection and cross-site scripting, but a standardized methodology to evaluate and benchmark such systems is not available. We outline several requirements for a testing and evaluation framework for these systems, and we introduce the concept of a benchmarking testbed, which automatically performs the evaluation in a standardized and reproducible way. By allowing benchmarks to draw from a corpus of installable modules which can be based on actual security vulnerabilities, members of the security community can continuously maintain and improve the benchmark, allowing it to be updated as threats and defenses evolve. We developed a prototype of this testbed and determined that the testbed should automate several common web testing tasks on behalf of its modules in order to ease module development. Although our experiences with the prototype suggest that developing such a testbed is viable, we identified several open questions related to benchmark coverage and performance measurement that should be resolved in order for the resulting benchmark to be useful to end users.}
,	keywords	= {SQL injection attack;Web application;Web intrusion prevention system;Web testing task;benchmarking testbed concept;cross-site scripting attck;module development;security vulnerability;Internet;security of data;}
,	doi	= {10.1109/Metrisec.2011.14}
}

@INPROCEEDINGS{metrisec2012surface
,	author	= {Stuckman, J. and Purtilo, J.}
,	title	= {Comparing and Applying Attack Surface Metrics}
,	booktitle	= {Proceedings of MetriSec: International Workshop on Security Measurements and Metrics}
,	year	= {2012}
}

@article{stuckman2011analyzing,
  title={Analyzing the wikisphere: Methodology and data to support quantitative wiki research},
  author={Stuckman, Jeffrey and Purtilo, James},
  journal={Journal of the American Society for Information Science and Technology},
  volume={62},
  number={8},
  pages={1564--1576},
  year={2011},
  publisher={Wiley Online Library}
}

@inproceedings{commandinjection
,	author	= {Su, Zhendong and Wassermann, Gary}
,	title	= {The essence of command injection attacks in web applications}
,	booktitle	= {Conference record of POPL '06: The 33rd ACM SIGPLAN-SIGACT symposium on Principles of programming languages}
,	year	= {2006}
,	pages	= {372--382}
,	publisher	= {ACM}
,	address	= {New York, NY, USA}
,	isbn	= {1-59593-027-2}
,	location	= {Charleston, South Carolina, USA}
,	numpages	= {11}
,	url	= {http://doi.acm.org/10.1145/1111037.1111070}
,	doi	= {http://doi.acm.org/10.1145/1111037.1111070}
,	acmid	= {1111070}
,	keywords	= {command injection attacks, grammars, parsing, runtime verification, web applications}
}

@ARTICLE{miningbugfinding
,	author	= {Williams, C.C. and Hollingsworth, J.K.}
,	title	= {Automatic mining of source code repositories to improve bug finding techniques}
,	journal	= {Software Engineering, IEEE Transactions on}
,	year	= {2005}
,	volume	= {31}
,	number	= {6}
,	pages	= { 466 - 480}
,	month	= {June}
,	abstract	= { We describe a method to use the source code change history of a software project to drive and help to refine the search for bugs. Based on the data retrieved from the source code repository, we implement a static source code checker that searches for a commonly fixed bug and uses information automatically mined from the source code repository to refine its results. By applying our tool, we have identified a total of 178 warnings that are likely bugs in the Apache Web server source code and a total of 546 warnings that are likely bugs in Wine, an open-source implementation of the Windows API. We show that our technique is more effective than the same static analysis that does not use historical data from the source code repository.}
,	keywords	= { Apache Web server; Windows API; automatic mining; bug finding technique; configuration control; data retrieval; debugging aids; historical data; open-source implementation; software project; source code repository; static analysis; static source code checker; testing tools; version control; Internet; application program interfaces; configuration management; data mining; file servers; program debugging; program diagnostics; program testing; public domain software;}
,	doi	= {10.1109/TSE.2005.63}
,	issn	= {0098-5589}
}

@INPROCEEDINGS{predictingdefectseclipse
,	author	= {Zimmermann, T. and Premraj, R. and Zeller, A.}
,	title	= {Predicting Defects for Eclipse}
,	booktitle	= {Proceedings of PROMISE'07: ICSE Workshop on Predictor Models in Software Engineering}
,	year	= {2007}
,	pages	= {9}
,	month	= {may}
,	abstract	= {We have mapped defects from the bug database of eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.}
,	keywords	= {Eclipse;bug database;common complexity metrics;defect prediction models;open-source projects;source code locations;program debugging;public domain software;}
,	doi	= {10.1109/PROMISE.2007.10}
}

